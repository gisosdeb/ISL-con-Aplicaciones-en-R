# Regresi√≥n Lineal (p.70)

---

<aside>
üéØ *Seg√∫n James et al. (2013), en el libro "An Introduction to Statistical Learning", la regresi√≥n lineal es un m√©todo estad√≠stico que se utiliza para modelar la relaci√≥n entre una variable dependiente y una o m√°s variables independientes. La variable dependiente es la variable que se est√° tratando de predecir, y las variables independientes son las variables que se utilizan para hacer la predicci√≥n. [AN INTRODUCTION TO STATISCTICAL LEARNING: with Applications with R](https://www.notion.so/AN-INTRODUCTION-TO-STATISCTICAL-LEARNING-with-Applications-with-R-b6f1d10032844592b50c82b4aadaadc0?pvs=21)*

</aside>

### 3.1 Regresi√≥n lineal simple

---

La regresi√≥n lineal se basa en la suposici√≥n de que la relaci√≥n entre la variable dependiente$\ Y$ y las variables independientes $\ X$ es **lineal**. Esto significa que la relaci√≥n entre las dos variables se puede modelar mediante la ecuaci√≥n de una [l√≠nea recta](https://www.notion.so/l-nea-recta-81091b9995254cb5aac1c0f8eb0d2d55?pvs=21):

$$
\begin{equation} Y\approx \beta_0+\beta_1X \end{equation}

$$

Donde $\approx$ deber√° interpretarse como **‚Äúes modelado aproximadamente‚Ä¶‚Äù** Tambi√©n, com√∫nmente, es bien dicho que: $Y$ **es regresado en** $X$.

$\beta_0$ y $\beta_1$ son constantes desconocidas que representan la **‚Äúintersecci√≥n‚Äù** y la ‚Äú**pendiente**‚Äù de una recta. Ambas ser√°n conocidas como **par√°metros del modelo**. As√≠ entonces tenemos que:

$$
\begin{equation} \hat y\approx \hat\beta_0+\hat\beta_1x\end{equation}

$$

ser√° nuestro **‚Äúmodelo de par√°metros‚Äù** donde una vez que usamos nuestros datos de entrenamiento para producir estimaciones para $\hat\beta_0$ y $\hat\beta_1$ podemos entonces hacer predicciones para cualquier valor de $\hat y=Y$ con base en $X=x$.

### 3.1.1 Estimaci√≥n de par√°metros o coeficientes

---

Para poder hacer predicciones podemos usar nuestros datos para poder hacer estimaciones de nuestros coeficientes. Ahora, si:

$$
\ (x_1,y_1), (x_2,y_2)\dots (x_n,y_n)

$$

representan $n$ pares ordenados de observaciones y cada uno representa un valor o medici√≥n de $Y$ a partir de $X$. En el dataset $‚ÄúAdvertising‚Äù$, cada uno de los datos representa el presupuesto designado para propaganda en la $TV$, $Radio$ y $Peri√≥dico$; y la cantidad de productos vendidos en 200 tiendas diferentes $(sales)$.

Nuestro objetivo entonces es estimar los coeficientes $\beta_0$ y $\beta_1$ que se ajusten mejor al modelo lineal $\hat y\approx \hat\beta_0+\hat\beta_1x$. En otras palabras necesitamos encontrar una recta de regresi√≥n ($least\ squares\ line$)$**\ ^1**$, ****la mejor, que se ajusta a los 200 datos de $TV \sim sales$ disponibles para este caso.

Para ajustar **un modelo de regresi√≥n lineal** a un conjunto de datos, uno de los m√©todos com√∫nmente utilizado es el de m√≠nimos cuadrados. El m√©todo de m√≠nimos cuadrados minimiza la suma de los cuadrados de las diferencias entre los valores observados de la variable dependiente y los valores predichos por el modelo.

Sabemos que $Y$ puede predecirse con $(2)$. Pero, si compramos el valor de la predicci√≥n con el valor de $X$ tendremos un error en los datos, un error residual$**\ ^2**$ $(RSS)$ que es igual a  $e=y_i-\hat y_i$. El $RSS$ total ser√° igual a: 

$$
RSS= e_1^2+e_2^2+\dots +e_n^2
$$

Si sustituimos a  $\hat y_i$ por $\hat\beta_0+\hat\beta_1x_1$ por $e$ tenemos una expresi√≥n equivalente:

$$
RSS=(y_1-\hat\beta_0-\hat\beta_1x_1)^2+\dots+(y_n-\hat\beta_0-\hat\beta_1x_n)^2
$$

El [m√©todo de los m√≠nimos cuadrados](https://www.notion.so/m-todo-de-los-m-nimos-cuadrados-3441aded2a684a97a5e7a6db43608a38?pvs=21) calcula a $\hat\beta_0$ y  $\hat\beta_1$ a partir de:

$$
\begin{split} & \hat\beta_1=\frac{\sum_{i=1}^{n}(x_i-\bar x)-(y_i-\bar y)}{\sum_{i=1}^{n}(x_i-\bar x)^2} \\\\ &\hat\beta_0=\bar y-\hat\beta\bar x \end{split}

$$

para $\bar y \equiv\frac {1}{n}\sum_{i=1}^{n} y_i$ y $\bar x \equiv\frac {1}{n}\sum_{i=1}^{n} x_i$ que en otras palabras son los promedios.

### 3.1.3 Evaluaci√≥n de la exactitud de los coeficientes

---

La relaci√≥n entre $X$ y $Y$ puede tomar la forma $Y=\ f(X)+\epsilon$ , para cualquier funci√≥n $f$ desconocida, y donde $\epsilon$ es la variaci√≥n en los datos que no se puede explicar por el modelo y que parte de una [distribuci√≥n normal con promedio igual a cero](https://www.notion.so/distribuci-n-normal-con-promedio-igual-a-cero-3a7861e89ce842e1bcd249089fda7e68?pvs=21). En otras palabras, el error $\epsilon$ es lo que hace al modelo interesante.

$$
\begin{equation} Y=2+3X+\epsilon \end{equation}
$$

En la vida real nunca vamos a poder acceder a todos los datos que forman parte de nuestro modelo. Siempre estaremos acotados a un conjunto de datos (datos de observaci√≥n) y de ah√≠ tendremos que realizar nuestras predicciones.

La ecuaci√≥n n√∫mero ($3$)$**\ ‚Äúleast\ squares\ line‚Äù**$ es la ecuaci√≥n de uno de los modelos, a partir de los coeficientes calculados $\hat \beta_0\$ y $\hat \beta_1$, que puede trata de imitar el verdadero comportamiento de$\ Y=2+3X$ la cual es nombrada$**\ ‚Äúpopulation\ regression\ line‚Äù$** y que generalmente es desconocida. Entonces, si, siempre tenemos acceso a diferentes conjuntos de datos de observaci√≥n, con los cuales pueden calcular diferentes rectas$\ ‚Äúleast\ squares\ lines‚Äù$, dichas rectas van a ser parecidas, pero la que nunca cambiar√° es la recta$\ ‚Äúpopulation\ regression\ line‚Äù$ porque es la recta que representa a la ecuaci√≥n.

Supongamos que estamos tratando de estimar el $\mu$ de una poblaci√≥n para unos datos aleatorios de $Y$. Desafortunadamente, $\mu$ siempre ser√° desconocida pero tenemos acceso a $n$ cantidad de datos de observaci√≥n de $Y$, $y_1\dots \ y_n$ con los cuales podemos **‚Äúestimar‚Äù** $\mu$. Podemos decir entonces que $\hat\mu = \bar{y}$ donde $\bar{y}=\frac {1}{n}\sum_{i=1}^{n} y_i$ o sea el promedio de la muestra. El promedio de la poblaci√≥n y de la muestra siempre van a ser diferentes, pero el promedio de la muestra puede ser una buena estimaci√≥n del de la poblaci√≥n.

Podemos decir que el problema de tratar de estimar a $\mu$ y el tratar de encontrar una l√≠nea de regresi√≥n a partir de los coeficientes $\hat \beta_0,\ \hat \beta_1$ es el mismo, pero existe un **sesgo**. Es decir que; esperar√≠amos que $\hat\mu$ fuera igual que $\mu$ para no existir ning√∫n sesgo, pero esto nunca pasa. Siempre existe una estimaci√≥n y una sobre estimaci√≥n de  $\hat\mu$ sobre  $\mu$. Esta estimaci√≥n puede ser inexacta debido a la aleatoriedad de los datos de observaci√≥n. En otras palabras eso es lo que representa el sesgo. Sin embargo, si pudi√©ramos promediar a varios $\hat\mu$  de diferentes conjuntos de datos podemos estar seguros que ese promedio ser√≠a igual que $\mu$. Por lo tanto, con este ejemplo, podemos decir que los par√°metros $\hat \beta_0\ y\ \hat \beta_1$, para diferentes conjuntos de datos, no ser√°n siempre los mismos, pero siempre se acercar√°n a los valores reales de  $\beta_0\ , \beta_1$. ¬øQu√© tanto?, podemos estimarlo a partir del $[‚Äústandard\ error‚Äù](https://www.notion.so/varianza-y-error-est-ndar-SE-c23bd9a072134549bb476b092621cdfd?pvs=21)$ $SE$($\hat\mu$).

$$
\begin{equation} Var(\hat\mu)=SE(\hat\mu)^2= \frac{\sigma^2}{n}\end{equation}

$$

Donde $\sigma^2$ es la [desviaci√≥n est√°ndar](https://www.notion.so/varianza-y-desviaci-n-est-ndar-sigma-ae141a6773474e3a8d9522907ae129ea?pvs=21) para cada una de las entradas  $y_1$de $Y$. El error est√°ndar lo que quiere decirnos es: ***‚Äúen qu√© proporci√≥n la estimaci√≥n para $\hat\mu$ difiere del verdadero valor de $\mu$*‚Äù.** Esta diferencia tambi√©n est√° acotada a la cantidad de datos observados, mientras m√°s datos se tenga estaremos m√°s cercanos al verdadero valor de $\mu$.

Del mismo modo podemos preguntarnos qu√© tan cercano es nuestro valor $\hat\beta_0$  y $\hat\beta_1$ del verdadero valor de $\beta_0$ y $\beta_1$. Para poder calcular el$‚Äústandard\ error‚Äù$ asociado a $\hat\beta_0$ y $\hat\beta_1$ tenemos que:

$$
\begin{equation} \begin{split}
& SE(\hat\beta_0)^2=\sigma^2\bigg[\frac{1}{n}+\frac{\bar x^2}{\sum_{x=1}^n(x_i-\bar x)^2}\bigg] \\\\ & SE(\hat\beta_1)^2=\frac{\sigma^2}{\sum_{x=i}^n(x_i-\bar x^2)} \end{split} \end{equation}
$$

Donde $[\sigma^2=Var(\epsilon)](https://www.notion.so/variancia-de-una-variable-aleatoria-epsilon-9ad349ab0cc04b88bcb67dbbb4e84f2f?pvs=21)$, en otras palabras es la variancia de una variable aleatoria $\epsilon_i$ y donde se considera que es m√°s deseable que los errores tengan una varianza en com√∫n y que no est√©n correlacionados. El error est√°ndar del estimador de la pendiente, $SE(\hatŒ≤_1)$, es menor cuando los $x_i$ est√°n m√°s dispersos. Esto significa que la estimaci√≥n de la pendiente ser√° m√°s precisa cuando los $x_i$ est√©n m√°s dispersos. Tambi√©n se puede ver que el $SE(\hatŒ≤_0)$ del intercepto es igual a la media $\bar y$ si el valor de $\bar x$ fuera igual a cero. En general, es importante tener en cuenta que la varianza del error [es un par√°metro desconocido](https://www.notion.so/varianza-del-error-como-par-metro-desconocido-87d1a450f2e84ec59b341caa7f330e19?pvs=21) que no podemos observar directamente. Sin embargo, podemos estimar la varianza del error a partir de los datos utilizando un estimador, como la varianza muestral. Este estimador para \sigma es conocido como $‚Äúresidual\ standard\ error‚Äù$ y est√° dado por $RSE=\sqrt{RSS/(n-2)}$. Estrictamente hablando; cuando $\sigma¬≤$ es calculado desde los datos es importante tener en cuenta que el error est√°ndar del estimador de la pendiente se debe escribir como $\widehat{SE}(\hat\beta_1)$.

Los errores est√°ndar pueden ser utilizados como intervalos de confianza. El intervalo de confianza se define como un rango de valores tal que con un intervalo del $95\%$ de probabilidad, el rango contendr√° el verdadero valor desconocido del par√°metro. El rango es definido en t√©rminos del l√≠mite superior e inferior de la muestra de los datos. Un intervalo de confianza de $95\%$ tiene la siguiente propiedad: si tomamos varias muestras y construimos sus intervalos de confianza para cada una de ellas, el $95\%$ de los intervalos de confianza contendr√°n el verdadero valor del par√°metro desconocido. Para una regresi√≥n lineal, el intervalo de confianza de un $95\%$ toma la siguiente forma:

$$
\begin{equation} \hat\beta_1 \pm2\cdot SE(\hat\beta_1)\end{equation}
$$

Hay un 95% de probabilidad de que en el intervalo:

$$
\begin{equation} \bigg[\hat\beta_1-2\cdot SE(\hat\beta_1),\hat\beta_1+2\cdot SE(\hat\beta_1)\bigg] \end{equation}
$$

contenga el verdadero valor de $\beta_1$. De manera similar, el intervalo de confianza para $\beta_0$ aproximadamente toma la forma:

$$
\begin{equation} \hat\beta_0 \pm2\cdot SE(\hat\beta_0)\end{equation}
$$

Para el caso de $‚ÄúAdvertising‚Äù$, el intervalo$**\ ^3**$ con un $95\%$ de confianza construido para $\beta_0$ fue de $[6.130,7.935]$, y para $\beta_1$ fue de $[0.042,0.053]$. Por lo tanto, podemos concluir que; sin ning√∫n presupuesto asignado a la publicidad de televisi√≥n, en promedio, el n√∫mero de art√≠culos vendidos rondar√°n entre $6, 130$ y $7, 935$ unidades. Del mismo modo, por cada $\$ 1,000$  asignados a la publicidad de televisi√≥n en promedio las ventas ser√°n entre 42 y 43 unidades. 

El $‚Äústandard\ error‚Äù$ tambi√©n se puede utilizar para plantear las pruebas de hip√≥tesis de los coeficientes. La m√°s com√∫n de todas es la de la hip√≥tesis nula

Regresi√≥n lineal de $sales \approx \beta_0+\beta_1*TV$

```r
# cargamos los datos en una variable llamada "df"

attach(df)
plot(TV,sales, pch=16)
lm.fit=lm(sales ~ TV,df)
abline(lwd=2,col="red",lm.fit)
```

![$**\ ^1**$ *Advertising scatter plot sales ~ TV*](Regresio%CC%81n%20Lineal%20(p%2070)%209204c911cfba460d975f7a5c7e3de8f5/Untitled.png)

$**\ ^1**$ *Advertising scatter plot sales ~ TV*

```r
plot(TV,sales, pch=20, col="red")
abline(lwd=2,col="blue",lm.fit)
# obtenemos las predicci√≥nes del modelo
predictions = predict(lm.fit)
segments(TV, sales, TV, predictions, lwd=0.3)
```

![$**\ ^2**$ *Cada segmento de l√≠nea de color gris representa un residuo.*](Regresio%CC%81n%20Lineal%20(p%2070)%209204c911cfba460d975f7a5c7e3de8f5/Untitled%201.png)

$**\ ^2**$ *Cada segmento de l√≠nea de color gris representa un residuo.*

```r
View(lm.fit)
```

![*resumen del modelo*](Regresio%CC%81n%20Lineal%20(p%2070)%209204c911cfba460d975f7a5c7e3de8f5/Untitled%202.png)

*resumen del modelo*

```r
coef(lm.fit)
```

```
(Intercept)          TV  
7.03259355  0.04753664
```

---

```r
confint(lm.fit)
```

```
						2.5 %     97.5 %
(Intercept) 6.12971927 7.93546783
TV          0.04223072 0.05284256
```

**Aspectos importantes**

- a
- b
- c

---

Definiciones a partir del autor.

<aside>
üéØ Un sistema de $m$ ecuaciones lineales en $n$ variables es un conjunto de $m$ ecuaciones, cada una de las cuales es lineal en las mismas $n$ variables. [FUNDAMENTOS DE √ÅLGEBRA LINEAL](https://www.notion.so/FUNDAMENTOS-DE-LGEBRA-LINEAL-555414b9943345aab4fd1ca90077b50f?pvs=21)

</aside>

$\begin{alignedat}{2}&
a_{11}x_1+ a_{12}x_2+a_{13}x_3+\dotsc+a_{1n}x_n=\ b_1\\&a_{21}x_1+ a_{22}x_2+a_{23}x_3+\dotsc+a_{2n}x_n=\ b_2\\&a_{31}x_1+ a_{32}x_2+a_{33}x_3+\dotsc+a_{3n}x_n=\ b_3\\\vdots\\&a_{m1}x_1+ a_{m2}x_2+a_{m3}x_3+\dotsc+a_{mn}x_n=\ b_m\end{alignedat}$

**NOTA: un sistema de ecuaciones puede tener:**

- exactamente una soluci√≥n.
- un n√∫mero infinito de soluciones (p.ej.) cuando se obtienen soluciones como $\ 0=0$ entre las ecuaciones de alg√∫n sistema.
- no tiene soluci√≥n (p.ej.) cuando se obtienen soluciones como $\ 0=2$ entre las ecuaciones de alg√∫n sistema.

[Recursos](Regresio%CC%81n%20Lineal%20(p%2070)%209204c911cfba460d975f7a5c7e3de8f5/Recursos%20ad88798d634d41c3817749b36b7472ad.csv)

**Aspectos importantes**

- a
- b
- c

---

Definiciones a partir del autor.

<aside>
üéØ Un sistema de $m$ ecuaciones lineales en $n$ variables es un conjunto de $m$ ecuaciones, cada una de las cuales es lineal en las mismas $n$ variables. [FUNDAMENTOS DE √ÅLGEBRA LINEAL](https://www.notion.so/FUNDAMENTOS-DE-LGEBRA-LINEAL-555414b9943345aab4fd1ca90077b50f?pvs=21)

</aside>

$\begin{alignedat}{2}&
a_{11}x_1+ a_{12}x_2+a_{13}x_3+\dotsc+a_{1n}x_n=\ b_1\\&a_{21}x_1+ a_{22}x_2+a_{23}x_3+\dotsc+a_{2n}x_n=\ b_2\\&a_{31}x_1+ a_{32}x_2+a_{33}x_3+\dotsc+a_{3n}x_n=\ b_3\\\vdots\\&a_{m1}x_1+ a_{m2}x_2+a_{m3}x_3+\dotsc+a_{mn}x_n=\ b_m\end{alignedat}$

**NOTA: un sistema de ecuaciones puede tener:**

- exactamente una soluci√≥n.
- un n√∫mero infinito de soluciones (p.ej.) cuando se obtienen soluciones como $\ 0=0$ entre las ecuaciones de alg√∫n sistema.
- no tiene soluci√≥n (p.ej.) cuando se obtienen soluciones como $\ 0=2$ entre las ecuaciones de alg√∫n sistema.